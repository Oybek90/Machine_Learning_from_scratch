{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\n\n\nh1{\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: gold;\n}\n\n\nbody, p {\n    font-family: ariel;\n    font-size: 15px;\n    color: charcoal;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n\nh4 {\n    padding: 0px;\n    margin: 0;\n    font-family: ariel;\n    color: purple;\n}\n</style>\n\"\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-08-26T04:35:32.249412Z","iopub.execute_input":"2022-08-26T04:35:32.250007Z","iopub.status.idle":"2022-08-26T04:35:32.257752Z","shell.execute_reply.started":"2022-08-26T04:35:32.249979Z","shell.execute_reply":"2022-08-26T04:35:32.256415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align = \"center\" style = \"font-family:serif\"> üß¨ Machine Learning Models from Scratch üß™ </h1>","metadata":{}},{"cell_type":"markdown","source":"<p style = \"font-size:17px\">Here I have tried to code simple versions of very famous and simple Machine Learning Models based on my understanding of their working. I have also tried to explain my work and visualize some of it using dummy datasets. I will try to add some more models too and update my work in the coming days.</p>\n\n<b style = \"font-size:17px\">Any feedback is appreciated and hope I can get your help to improve these models and my understanding of their working.</b>","metadata":{}},{"cell_type":"markdown","source":"<h2 >üìå Models</h2>\n<ul style = \"font-size:17px\">\n    <li><a href=\"https://www.kaggle.com/code/varunnagpalspyz/ml-models-code-from-scratch#LinReg\">Linear Regression</a></li>\n    <ul>\n        <li><a href=\"https://www.kaggle.com/code/varunnagpalspyz/ml-models-code-from-scratch#SGD\">Stochastic Gradient Descent</a></li>\n        <li><a href=\"https://www.kaggle.com/code/varunnagpalspyz/ml-models-code-from-scratch#OLS\">Ordinary Least Squares</a></li>\n    </ul>\n    <li><a href=\"https://www.kaggle.com/code/varunnagpalspyz/ml-models-code-from-scratch#LogReg\">Logistic Regression</a></li>\n    <li><a href=\"https://www.kaggle.com/code/varunnagpalspyz/ml-models-code-from-scratch#KMeans\">KMeans Clustering</a></li>\n    <li><a href=\"https://www.kaggle.com/code/varunnagpalspyz/ml-models-code-from-scratch#KNN\">KNN</a></li>\n    <li><a href=\"https://www.kaggle.com/code/varunnagpalspyz/ml-models-code-from-scratch#GNB\">Gaussian Naive Bayes</a></li>\n    <li><a href=\"https://www.kaggle.com/code/varunnagpalspyz/ml-models-code-from-scratch#DTC\">Decision Tree Classifier</a></li> \n    <li><a href=\"https://www.kaggle.com/code/varunnagpalspyz/ml-models-code-from-scratch#RFC\">Random Forest Classifier</a></li>\n</ul>\n\n***","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:43:38.442649Z","iopub.execute_input":"2022-08-26T04:43:38.443047Z","iopub.status.idle":"2022-08-26T04:43:38.454492Z","shell.execute_reply.started":"2022-08-26T04:43:38.443013Z","shell.execute_reply":"2022-08-26T04:43:38.452693Z"}}},{"cell_type":"markdown","source":"## üì¶ Importing Libraries","metadata":{}},{"cell_type":"code","source":"# Basic Imports\nimport numpy as np\nimport random\n\n#to visualize and verify my work\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# To verify my work on dummy datasets using sklearn\nfrom sklearn.datasets import make_blobs,make_regression,make_classification\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:32.259468Z","iopub.execute_input":"2022-08-26T04:35:32.260161Z","iopub.status.idle":"2022-08-26T04:35:32.269594Z","shell.execute_reply.started":"2022-08-26T04:35:32.260134Z","shell.execute_reply":"2022-08-26T04:35:32.268669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  üÜò Helper Functions","metadata":{}},{"cell_type":"code","source":"''' Utils '''\n\ndef mode(ls):\n    ''' function to find the mode of a list '''\n    \n    # dictionary to keep count of each value\n    counts = {}\n    # iterate through the list\n    for item in ls:\n        if item in counts:\n            counts[item] += 1\n        else:\n            counts[item] = 1\n    # get the keys with the max counts\n    return [key for key in counts.keys() if counts[key] == max(counts.values())]\n\ndef sigmoid(z):\n    ''' function to find sigmoid '''\n    return 1/(1+np.exp(-z))\n\ndef mse(y,yhat):\n    ''' Mean Squared Error loss function '''\n    return np.average((y - yhat) ** 2, axis=0)\n\ndef logloss():\n    ''' Binary Cross Entropy loss function '''\n    pass","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:32.271322Z","iopub.execute_input":"2022-08-26T04:35:32.27218Z","iopub.status.idle":"2022-08-26T04:35:32.282565Z","shell.execute_reply.started":"2022-08-26T04:35:32.272146Z","shell.execute_reply":"2022-08-26T04:35:32.281674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align = \"center\" id = \"LinReg\" >üìè Linear Regression</h1>","metadata":{}},{"cell_type":"markdown","source":"<h2 id = \"SGD\"> Stochastic Gradient Descent </h2>","metadata":{}},{"cell_type":"markdown","source":"<b>For more information on Gradient Descent check out:</b> https://www.youtube.com/watch?v=ORyfPJypKuU&t=2s","metadata":{}},{"cell_type":"code","source":"class SGDRegressor:\n    \n    def __init__(self,epochs = 1000,lr = 0.001):\n        ''' initializing the epochs and learning rate parameters '''\n        \n        # defining number of epochs i.e. number of times we loop through the entire data\n        self.epochs = epochs\n        # defining learning rate for gradient descent\n        self.lr = lr \n        \n    def fit(self,x,y):\n        ''' function to train the model '''\n        \n        # taking the number of examples and number of features in x\n        self.num_samples,self.num_features = x.shape \n        # weights\n        self.w = np.zeros(self.num_features) \n        # bias\n        self.b = 0 \n        # updating weights after every example in every epoch\n        for i in range(self.epochs):\n            self.update_weights(x,y) \n        \n    def predict(self,x):\n        ''' Predicting the output based on weights and biases set after training '''\n        return np.dot(x,self.w)+self.b \n    \n    def update_weights(self,x,y):\n        ''' Training and updating the weights based on Gradient Descent '''\n        \n        for i in range(self.num_samples):\n            row = x[i].T\n            #taking prediction from each sample in the data to update the w&b\n            ypred = self.predict(row)\n            # calculating update gradients\n            dw = -2*np.dot(row,y[i]-ypred) \n            db = - 2*np.sum(y[i]-ypred) \n            # applying gradient descent to update the weights and bias\n            self.w = self.w - self.lr*dw \n            self.b = self.b - self.lr*db","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:32.283627Z","iopub.execute_input":"2022-08-26T04:35:32.284469Z","iopub.status.idle":"2022-08-26T04:35:32.295731Z","shell.execute_reply.started":"2022-08-26T04:35:32.284443Z","shell.execute_reply":"2022-08-26T04:35:32.29494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id = \"OLS\"> Normal Equations (Ordinary Least Squares)</h2>","metadata":{}},{"cell_type":"markdown","source":"<b>For more information regarding the derivation of these formulae check out:</b> https://www.youtube.com/playlist?list=PLKnIA16_Rmva-wY_HBh1gTH32ocu2SoTr","metadata":{}},{"cell_type":"code","source":"class LinearRegression:\n    \n    def __init__(self):\n        ''' initializing the weights and biases parameter '''\n        \n        self.weights_ = None\n        self.bias_ = None\n    \n    def fit(self,x,y):\n        ''' function to train the model '''\n    \n        # inserting a column of 1s to represent the bias\n        x = np.insert(x,0,1,axis = 1)\n        #calculating the weights and bias based on mathematical formulae\n        coefs = np.linalg.inv(np.dot(x.T,x)).dot(x.T).dot(y)\n        self.weights_ = coefs[1:]\n        self.bias_ = coefs[0]\n    \n    def predict(self,xtest):\n        ''' Predicting the output based on weights and biases set after training '''\n        return np.dot(xtest,self.weights_) + self.bias_","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:32.297337Z","iopub.execute_input":"2022-08-26T04:35:32.297715Z","iopub.status.idle":"2022-08-26T04:35:32.315141Z","shell.execute_reply.started":"2022-08-26T04:35:32.297691Z","shell.execute_reply":"2022-08-26T04:35:32.314465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dummy Dataset","metadata":{}},{"cell_type":"code","source":"x,y = make_regression(n_samples=100, n_features=1, n_targets=1, noise=50, random_state=42)\nplt.scatter(x,y)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:32.316624Z","iopub.execute_input":"2022-08-26T04:35:32.317066Z","iopub.status.idle":"2022-08-26T04:35:32.473201Z","shell.execute_reply.started":"2022-08-26T04:35:32.31704Z","shell.execute_reply":"2022-08-26T04:35:32.472455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgd = SGDRegressor()\nsgd.fit(x,y)\nypred_sgd = sgd.predict(x)\n\nlr = LinearRegression()\nlr.fit(x,y)\nypred_lr = lr.predict(x)\n\nplt.figure(figsize = (20,5))\nplt.subplot(1,2,1)\nplt.scatter(x,y, color = 'red' )\nplt.plot(x,ypred_sgd, color = 'blue',label = 'sgd')\nplt.legend()\nplt.subplot(1,2,2)\nplt.scatter(x,y, color = 'red' )\nplt.plot(x,ypred_lr, color = 'green',label = 'ols')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:32.475464Z","iopub.execute_input":"2022-08-26T04:35:32.475721Z","iopub.status.idle":"2022-08-26T04:35:34.228862Z","shell.execute_reply.started":"2022-08-26T04:35:32.475697Z","shell.execute_reply":"2022-08-26T04:35:34.227989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align = \"center\" id = \"LogReg\"> üñá Logistic Regression</h1>","metadata":{}},{"cell_type":"code","source":"# Similar to SGDRegressor Implementation but tailored to binary classification with the help of sigmoid function\nclass LogisticRegression:\n    \n    def __init__(self,epochs=1000,lr=0.001):\n        self.epochs = epochs\n        self.lr = lr\n    \n    def fit(self,x,y):\n        self.m,self.n = x.shape\n        self.w = np.zeros(self.n)\n        self.b = 0\n        for i in range(self.epochs):\n            self.update_weights(x,y)\n    \n    def predict_proba(self,xtest):\n        return sigmoid(np.dot(xtest,self.w)+self.b)\n    \n    def predict(self,xtest):\n        ypred = []\n        for i in range(xtest.shape[0]):\n            if sigmoid(np.dot(xtest[i],self.w)+self.b) >=0.5:\n                ypred.append(1)\n            else:\n                ypred.append(0)\n        return ypred\n    \n    def update_weights(self,x,y):\n        for i in range(self.m):\n            row = x[i].T\n            yhat = self.predict_proba(row)\n            dw = -2*np.dot(row,y[i]-yhat)\n            db = - 2*np.sum(y[i]-yhat)  \n            self.w = self.w - self.lr*dw\n            self.b = self.b - self.lr*db","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:34.231063Z","iopub.execute_input":"2022-08-26T04:35:34.231534Z","iopub.status.idle":"2022-08-26T04:35:34.240998Z","shell.execute_reply.started":"2022-08-26T04:35:34.231507Z","shell.execute_reply":"2022-08-26T04:35:34.240316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dummy Dataset","metadata":{}},{"cell_type":"code","source":"x, y = make_classification(n_samples=100, n_features=2, n_informative=1,n_redundant=0,\n                           n_classes=2, n_clusters_per_class=1, random_state=42,hypercube=False,class_sep=20)\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:34.242097Z","iopub.execute_input":"2022-08-26T04:35:34.242901Z","iopub.status.idle":"2022-08-26T04:35:34.257732Z","shell.execute_reply.started":"2022-08-26T04:35:34.242872Z","shell.execute_reply":"2022-08-26T04:35:34.257069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(xtrain,ytrain)\nypred = lr.predict(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:34.258784Z","iopub.execute_input":"2022-08-26T04:35:34.259804Z","iopub.status.idle":"2022-08-26T04:35:36.08184Z","shell.execute_reply.started":"2022-08-26T04:35:34.259745Z","shell.execute_reply":"2022-08-26T04:35:36.08074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.084498Z","iopub.execute_input":"2022-08-26T04:35:36.084771Z","iopub.status.idle":"2022-08-26T04:35:36.090948Z","shell.execute_reply.started":"2022-08-26T04:35:36.084748Z","shell.execute_reply":"2022-08-26T04:35:36.090398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(ytest)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.091949Z","iopub.execute_input":"2022-08-26T04:35:36.092832Z","iopub.status.idle":"2022-08-26T04:35:36.105329Z","shell.execute_reply.started":"2022-08-26T04:35:36.092798Z","shell.execute_reply":"2022-08-26T04:35:36.104349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align = \"center\" id = \"KMeans\"> üî¢ KMeans Clustering</h1> ","metadata":{}},{"cell_type":"markdown","source":"## Steps \n\n<ul>\n<li>Randomly initialize centroids from the sample\n<li>Assign Clusters based on euclidean distance from centroids\n<li>Move centroids based on new mean position\n</ul>","metadata":{}},{"cell_type":"code","source":"class KMeans:\n    \n    def __init__(self,n_clusters=5,max_iters=100):\n        self.n_clusters = n_clusters\n        self.max_iters = max_iters\n        self.centroids = None\n        \n    def fit_predict(self,x):\n        # Randomly initialize centroids\n        self.centroids = x[random.sample(range(x.shape[0]),self.n_clusters)]\n        for i in range(self.max_iters):\n            # assigning clusters\n            clusters = self.assign_clusters(x)\n            prev_centroids = self.centroids\n            # Moving centroids\n            self.centroids = self.move_centroids(x,clusters)\n            # Checking if centroids have changed \n            if (prev_centroids == self.centroids).all():\n                break\n        return clusters\n        \n    def assign_clusters(self,x):\n        clusters = []\n        for sample in x:\n            distances = []\n            for centroid in self.centroids:\n                distances.append(np.sqrt(np.dot(sample-centroid,sample-centroid)))\n            index = distances.index(min(distances))\n            clusters.append(index)\n        return np.array(clusters)\n    \n    def move_centroids(self,x,clusters):\n        cluster_types = np.unique(clusters)\n        new_centroids = []\n        for cluster in cluster_types:\n            new_centroids.append(x[clusters == cluster].mean(axis = 0))\n        return np.array(new_centroids)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.106606Z","iopub.execute_input":"2022-08-26T04:35:36.106898Z","iopub.status.idle":"2022-08-26T04:35:36.116496Z","shell.execute_reply.started":"2022-08-26T04:35:36.106874Z","shell.execute_reply":"2022-08-26T04:35:36.115714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dummy Dataset","metadata":{}},{"cell_type":"code","source":"centroids = [(-5,-5),(5,5),(-10,-10),(0,0)]\ncluster_std = [1,1,1,1]\n\nX,y = make_blobs(n_samples=100,cluster_std=cluster_std,centers=centroids,n_features=2,random_state=42)\nplt.scatter(X[:,0],X[:,1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.11817Z","iopub.execute_input":"2022-08-26T04:35:36.118829Z","iopub.status.idle":"2022-08-26T04:35:36.262715Z","shell.execute_reply.started":"2022-08-26T04:35:36.118795Z","shell.execute_reply":"2022-08-26T04:35:36.261976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"km = KMeans(n_clusters=4,max_iters = 1000)\ny_means = km.fit_predict(X)\n\nplt.scatter(X[y_means == 0,0],X[y_means == 0,1],color='red')\nplt.scatter(X[y_means == 1,0],X[y_means == 1,1],color='blue')\nplt.scatter(X[y_means == 2,0],X[y_means == 2,1],color='green')\nplt.scatter(X[y_means == 3,0],X[y_means == 3,1],color='yellow')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:56.813515Z","iopub.execute_input":"2022-08-26T04:35:56.814007Z","iopub.status.idle":"2022-08-26T04:35:56.966556Z","shell.execute_reply.started":"2022-08-26T04:35:56.813982Z","shell.execute_reply":"2022-08-26T04:35:56.965809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align = \"center\" id = \"KNN\"> üî™ KNN</h1>","metadata":{}},{"cell_type":"code","source":"class KNearestNeighbors:\n    \n    def __init__(self,k=2):\n        self.k = k\n        \n    def fit(self,x,y):\n        if x.shape[0]-1<self.k:\n            print('Error, value of K greater than number of samples')\n        else:\n            self.x = x\n            self.y = y\n        \n    def predict(self,xtest):\n        if xtest.shape[0]-1<self.k:\n            print('Error, value of K greater than number of samples')\n        else:\n            ypred = []\n            for i in range(xtest.shape[0]):\n                distances = {}\n                counter = 0\n                for j in range(self.x.shape[0]):\n                    distances[counter] = np.sqrt(np.dot(xtest[i]-self.x[j],xtest[i]-self.x[j]))\n                    counter = counter + 1\n                distances = dict(sorted(distances.items(), key=lambda x:x[1]))\n                keys = list(distances.keys())[:self.k]\n                ypred.append(mode(list(self.y[keys]))[0]) \n            return ypred","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.418022Z","iopub.execute_input":"2022-08-26T04:35:36.418558Z","iopub.status.idle":"2022-08-26T04:35:36.427558Z","shell.execute_reply.started":"2022-08-26T04:35:36.418523Z","shell.execute_reply":"2022-08-26T04:35:36.426649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dummy Dataset","metadata":{}},{"cell_type":"code","source":"x,y = make_classification(n_samples=100, n_features=5, n_classes=2, random_state=42)\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.428914Z","iopub.execute_input":"2022-08-26T04:35:36.429149Z","iopub.status.idle":"2022-08-26T04:35:36.445955Z","shell.execute_reply.started":"2022-08-26T04:35:36.429126Z","shell.execute_reply":"2022-08-26T04:35:36.444717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNearestNeighbors()\nknn.fit(xtrain,ytrain)\nypred = knn.predict(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.447129Z","iopub.execute_input":"2022-08-26T04:35:36.44761Z","iopub.status.idle":"2022-08-26T04:35:36.463996Z","shell.execute_reply.started":"2022-08-26T04:35:36.447579Z","shell.execute_reply":"2022-08-26T04:35:36.463334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicted Results\nypred","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.465154Z","iopub.execute_input":"2022-08-26T04:35:36.465441Z","iopub.status.idle":"2022-08-26T04:35:36.471135Z","shell.execute_reply.started":"2022-08-26T04:35:36.465417Z","shell.execute_reply":"2022-08-26T04:35:36.470454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Actual Results\nlist(ytest)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.472052Z","iopub.execute_input":"2022-08-26T04:35:36.473136Z","iopub.status.idle":"2022-08-26T04:35:36.48543Z","shell.execute_reply.started":"2022-08-26T04:35:36.473084Z","shell.execute_reply":"2022-08-26T04:35:36.484358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align = \"center\" id = \"GNB\"> üì® Gaussian Naive Bayes</h1>","metadata":{}},{"cell_type":"code","source":"class GaussianNaiveBayes:\n    \n    def __init__(self):\n        self.nclasses = None\n        self.mean = {}\n        self.var = {}\n        self.prior = {}\n        self.eps = 1e-5 #stability factor (saw in sklearn implementation)\n    \n    def fit(self,x,y):\n        self.m,self.n = x.shape\n        self.nclasses = len(np.unique(y))\n        for clas in range(self.nclasses):\n            x_c = x[y==clas]\n            self.mean[str(clas)] = np.mean(x_c,axis = 0)\n            self.var[str(clas)] = np.var(x_c,axis = 0)\n            self.prior[str(clas)] = x_c.shape[0]/self.m\n            \n    \n    def predict(self,xtest):\n        probs = np.zeros((xtest.shape[0],self.nclasses))\n        \n        for clas in range(self.nclasses):\n            prior = self.prior[str(clas)]\n            probs_clas = self.density_function(xtest,self.mean[str(clas)],self.var[str(clas)])\n            probs[:,clas] = np.log(prior) + probs_clas\n        \n        return np.argmax(probs,axis=1)\n            \n    # Gaussian density function used to find probability of a feature value (continous feature) given the class\n    def density_function(self,x,mean,var):\n        a = (-1/2)*(np.log(2*np.pi))*(self.n) -(1/2)*(np.sum(np.log(var+self.eps)))\n        b = (-1/2)*np.sum(np.power(x-mean,2)/(var+self.eps),1)\n        return a+b\n    ","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.486595Z","iopub.execute_input":"2022-08-26T04:35:36.487566Z","iopub.status.idle":"2022-08-26T04:35:36.501985Z","shell.execute_reply.started":"2022-08-26T04:35:36.487504Z","shell.execute_reply":"2022-08-26T04:35:36.50096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x,y = make_classification(n_samples=100, n_features=10, n_classes=2, random_state=69)\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.503441Z","iopub.execute_input":"2022-08-26T04:35:36.504027Z","iopub.status.idle":"2022-08-26T04:35:36.518807Z","shell.execute_reply.started":"2022-08-26T04:35:36.50399Z","shell.execute_reply":"2022-08-26T04:35:36.517777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gnb = GaussianNaiveBayes()\ngnb.fit(xtrain,ytrain)\nypred = gnb.predict(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.523405Z","iopub.execute_input":"2022-08-26T04:35:36.523856Z","iopub.status.idle":"2022-08-26T04:35:36.533614Z","shell.execute_reply.started":"2022-08-26T04:35:36.523809Z","shell.execute_reply":"2022-08-26T04:35:36.532619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicted results\nypred","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.534718Z","iopub.execute_input":"2022-08-26T04:35:36.535532Z","iopub.status.idle":"2022-08-26T04:35:36.546679Z","shell.execute_reply.started":"2022-08-26T04:35:36.535506Z","shell.execute_reply":"2022-08-26T04:35:36.545705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Actual results\nlist(ytest)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.547853Z","iopub.execute_input":"2022-08-26T04:35:36.548482Z","iopub.status.idle":"2022-08-26T04:35:36.561792Z","shell.execute_reply.started":"2022-08-26T04:35:36.548454Z","shell.execute_reply":"2022-08-26T04:35:36.560379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align = \"center\" id = \"DTC\"> üå¥ Decision Tree Classifier</h1>","metadata":{}},{"cell_type":"markdown","source":"<b>For more information regarding this code checkout this playlist:</b> https://www.youtube.com/playlist?list=PLM8wYQRetTxAl5FpMIJCcJbfZjSB0IeC_","metadata":{}},{"cell_type":"code","source":"class Node:\n    ''' Node Class ''' \n\n    def __init__(self,feature = None,feature_value = None,left = None,right = None,info_gain = None,value = None):\n        ''' constructor '''\n    \n        #decision node\n        self.left = left\n        self.right = right\n        self.info_gain = info_gain\n        self.feature = feature\n        self.feature_value = feature_value\n        \n        #leaf node\n        self.value = value\n\nclass DecisionTreeClassifier:\n    ''' Tree Class '''\n    \n    def __init__(self,min_samples_split = 2,max_depth = None):\n        ''' constructor '''\n    \n        #initializing root of the tree\n        self.tree = None\n        \n        #initializing stopping condition (sklearn default values used)\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        \n    def split(self,data,feature,feature_value):\n        ''' split a node based on a particular feature and its threshold'''\n        \n        left = np.array([row for row in data if row[feature]<=feature_value])\n        right = np.array([row for row in data if row[feature]>feature_value])\n        \n        return left,right\n    \n    def build_tree(self,data,depth = 0):\n        ''' recursive function to build the tree'''\n        \n        x, y = data[:,:-1], data[:,-1]\n        num_samples,num_features = np.shape(x)\n        \n        # split until stopping conditions are met\n        if num_samples >= self.min_samples_split and depth <=self.max_depth:\n            # find the best split\n            best_split = self.get_best_split(data,num_features,num_samples)\n            # check if information gain is positive\n            if best_split['info_gain'] > 0:\n                left = self.build_tree(best_split['left'],depth+1)\n                right = self.build_tree(best_split['right'],depth+1)\n                #return decision node\n                return Node(best_split['feature'],best_split['feature_value'],left,right,best_split['info_gain'])\n            \n        # compute leaf node\n        leaf_value = self.calculate_leaf_value(y)\n        # return leaf node\n        return Node(value = leaf_value)\n    \n    def get_best_split(self,data,num_features,num_samples):\n        ''' to find the best split '''    \n    \n        # dictionary to store the best split\n        best_split = {}\n        max_info_gain = -float(\"inf\")\n        \n        # loop over all the features\n        for feature in range(num_features):\n            feature_values = data[:,feature]\n            unique_values = np.unique(feature_values)\n            # loop over all the values of the feature\n            for value in unique_values:\n                # get current split\n                left,right = self.split(data, feature, value)\n                # check if childs are not null\n                if len(left)>0 and len(right)>0:\n                    y, left_y, right_y = data[:, -1], left[:, -1], right[:, -1]\n                    # compute information gain\n                    curr_info_gain = self.information_gain(y, left_y, right_y,\"entropy\")\n                    # update the best split if needed\n                    if curr_info_gain>max_info_gain:\n                        best_split[\"feature\"] = feature\n                        best_split[\"feature_value\"] = value\n                        best_split[\"left\"] = left\n                        best_split[\"right\"] = right\n                        best_split[\"info_gain\"] = curr_info_gain\n                        max_info_gain = curr_info_gain\n                        \n        # return best split\n        return best_split\n                \n    \n    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n        ''' function to compute information gain '''\n        \n        weight_l = len(l_child) / len(parent)\n        weight_r = len(r_child) / len(parent)\n        if mode==\"gini\":\n            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n        else:\n            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n        return gain\n    \n    def entropy(self, y):\n        ''' function to compute entropy '''\n        \n        class_labels = np.unique(y)\n        entropy = 0\n        for label in class_labels:\n            p_class = len(y[y == label]) / len(y)\n            entropy += -p_class * np.log2(p_class)\n        return entropy\n    \n    def gini_index(self, y):\n        ''' function to compute gini index '''\n        \n        class_labels = np.unique(y)\n        gini = 0\n        for label in class_labels:\n            p_class = len(y[y == label]) / len(y)\n            gini += p_class**2\n        return 1 - gini\n    \n    def calculate_leaf_value(self, y):\n        ''' function to compute leaf node '''\n        \n        y = list(y)\n        return max(y, key=y.count)\n    \n    def fit(self,x,y):\n        ''' function to train the tree '''  \n    \n        data = np.concatenate((x,y),axis = 1)\n        self.tree = self.build_tree(data)\n    \n    def predict(self, X):\n        ''' function to predict new dataset '''\n        \n        predictions = [self.make_prediction(x,self.tree) for x in X]\n        return predictions\n    \n    def make_prediction(self, x,tree):\n        ''' function to predict a single data point '''\n        \n        if tree.value != None: \n            return tree.value\n        feature_val = x[tree.feature]\n        if feature_val<=tree.feature_value:\n            return self.make_prediction(x, tree.left)\n        else:\n            return self.make_prediction(x, tree.right)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.563231Z","iopub.execute_input":"2022-08-26T04:35:36.563569Z","iopub.status.idle":"2022-08-26T04:35:36.587331Z","shell.execute_reply.started":"2022-08-26T04:35:36.563543Z","shell.execute_reply":"2022-08-26T04:35:36.586523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x,y = make_classification(n_samples=100, n_features=10, n_classes=2, random_state=45)\ny = y.reshape(-1,1)\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.588119Z","iopub.execute_input":"2022-08-26T04:35:36.588396Z","iopub.status.idle":"2022-08-26T04:35:36.606706Z","shell.execute_reply.started":"2022-08-26T04:35:36.588371Z","shell.execute_reply":"2022-08-26T04:35:36.605184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = DecisionTreeClassifier(min_samples_split=3, max_depth=3)\ndt.fit(xtrain,ytrain)\nypred = dt.predict(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.608066Z","iopub.execute_input":"2022-08-26T04:35:36.608331Z","iopub.status.idle":"2022-08-26T04:35:36.868561Z","shell.execute_reply.started":"2022-08-26T04:35:36.608307Z","shell.execute_reply":"2022-08-26T04:35:36.866628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.predict(np.expand_dims(xtest[0],axis = 0))[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.871222Z","iopub.execute_input":"2022-08-26T04:35:36.871588Z","iopub.status.idle":"2022-08-26T04:35:36.88091Z","shell.execute_reply.started":"2022-08-26T04:35:36.871561Z","shell.execute_reply":"2022-08-26T04:35:36.879091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Actual results\nfor i in ytest:\n    print(i[0],end = ' ')","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.883188Z","iopub.execute_input":"2022-08-26T04:35:36.883841Z","iopub.status.idle":"2022-08-26T04:35:36.894691Z","shell.execute_reply.started":"2022-08-26T04:35:36.883802Z","shell.execute_reply":"2022-08-26T04:35:36.893372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicted results\nfor i in ypred:\n    print(int(i),end = ' ')","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.89595Z","iopub.execute_input":"2022-08-26T04:35:36.896398Z","iopub.status.idle":"2022-08-26T04:35:36.907363Z","shell.execute_reply.started":"2022-08-26T04:35:36.896372Z","shell.execute_reply":"2022-08-26T04:35:36.906279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 align = \"center\" id = \"RFC\"> üå≥üå≥ Random Forest Classifier üå≤üå≤ </h1>","metadata":{}},{"cell_type":"code","source":"class RandomForestClassifier():\n    \n    def __init__(self, n_trees = 10 , min_samples_split = 2, max_depth = None,max_features = None):\n        ''' Initializing the base parameters '''\n        \n        self.n_trees = n_trees\n        self.min_samples_split =  min_samples_split\n        self.max_depth = max_depth\n        self.max_features = max_features\n        self.trees = [self.create_tree() for _ in range(self.n_trees)]\n        \n    def sample(self,x, y):\n        ''' Function used for boostrap sampling and random feature selection '''\n        \n        # Setting the max_features parameter for feature selection\n        num_samples, num_features = x.shape\n        if self.max_features == 'sqrt':\n            self.max_features = np.sqrt(num_features)\n        elif self.max_features == 'log':\n            self.max_features = np.log2(num_features)\n        elif self.max_features == None:\n            self.max_features = num_features\n            \n        # Sample with replacement\n        sample = np.random.choice(a=num_samples, size=num_samples, replace=True)\n        # Random Feature selection\n        col_sample = np.random.choice(a = num_features,size = self.max_features,replace = False)\n        x = x[sample]\n        return x[:,col_sample], y[sample]\n        \n    def create_tree(self):\n        ''' Function to create the homogeneous weak learners '''\n        return DecisionTreeClassifier(max_depth = self.max_depth, min_samples_split = self.min_samples_split)\n    \n    def fit(self, x, y):   \n        ''' Function to train the multiple trees '''\n        \n        for tree in self.trees:\n            sample_x,sample_y = self.sample(x,y)\n            tree.fit(sample_x,sample_y) \n    \n    def predict(self, xtest):\n        ''' mode of the predictions from each tree '''\n        \n        predictions = []\n        for row in xtest:\n            predictions.append(mode([tree.predict(np.expand_dims(row,axis = 0))[0] for tree in self.trees])[0])\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.908579Z","iopub.execute_input":"2022-08-26T04:35:36.909646Z","iopub.status.idle":"2022-08-26T04:35:36.922948Z","shell.execute_reply.started":"2022-08-26T04:35:36.909581Z","shell.execute_reply":"2022-08-26T04:35:36.921571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x,y = make_classification(n_samples=100, n_features=10, n_classes=2, random_state=60)\ny = y.reshape(-1,1)\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.924867Z","iopub.execute_input":"2022-08-26T04:35:36.92535Z","iopub.status.idle":"2022-08-26T04:35:36.944925Z","shell.execute_reply.started":"2022-08-26T04:35:36.925311Z","shell.execute_reply":"2022-08-26T04:35:36.943767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(min_samples_split=3, max_depth=3)\nrf.fit(xtrain,ytrain)\nypred = rf.predict(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:36.945987Z","iopub.execute_input":"2022-08-26T04:35:36.946892Z","iopub.status.idle":"2022-08-26T04:35:38.482388Z","shell.execute_reply.started":"2022-08-26T04:35:36.946857Z","shell.execute_reply":"2022-08-26T04:35:38.481441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Actual results\nfor i in ytest:\n    print(i[0],end = ' ')","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:38.484173Z","iopub.execute_input":"2022-08-26T04:35:38.484561Z","iopub.status.idle":"2022-08-26T04:35:38.490492Z","shell.execute_reply.started":"2022-08-26T04:35:38.484527Z","shell.execute_reply":"2022-08-26T04:35:38.489675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicted results\nfor i in ypred:\n    print(int(i),end = ' ')","metadata":{"execution":{"iopub.status.busy":"2022-08-26T04:35:38.491527Z","iopub.execute_input":"2022-08-26T04:35:38.491889Z","iopub.status.idle":"2022-08-26T04:35:38.503793Z","shell.execute_reply.started":"2022-08-26T04:35:38.49186Z","shell.execute_reply":"2022-08-26T04:35:38.502859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gradient Boosting and SVM maybe..","metadata":{}}]}